# üìä Supervised Machine Learning ‚Äî Regression

Welcome to the **Supervised Regression** section of my AI learning journey!  
This folder covers the complete theory and hands-on implementation of regression models using Python, from the very basics to regularization and model evaluation. üöÄ

---

## üìö Topics Covered

Each topic below is backed with Python scripts, Jupyter notebooks, or Markdown notes.  
You can explore, run, or modify the code to understand how supervised regression truly works.

---

### ‚úÖ Basics of Regression

- **`simple-linear-regression.py`**  
  ### **Linear Regression**

- Linear Regression is a supervised machine learning algorithm used to model the relationship between a **dependent variable** (target) and an **independent variable** (feature).
- In **single variable linear regression**, there is only **one input feature** and **one output variable**.
- The model aims to fit a **line of best fit** using the equation:Y=mX+c

   ![Regression Graph](https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg)

    
    Y = m X + c
    
    Where:
    
    - Y: Predicted output (target)
    - X: Input feature
    - m: Slope of the line (coefficient)
    - c: Y-intercept (constant)
 
  

## üìò Multiple Linear Regression

**Multiple Linear Regression** is a statistical technique that models the relationship between one **dependent variable** and two or more **independent variables**.

### üßÆ Equation:
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô + Œµ

![Image Description](https://miro.medium.com/v2/resize:fit:560/1*HLN6FxlXrzDtYN0KAlom4A.png)

### üìå Where:
- **y**: Dependent variable  
- **x‚ÇÅ, x‚ÇÇ, ..., x‚Çô**: Independent variables  
- **Œ≤‚ÇÄ**: Intercept  
- **Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çô**: Coefficients of the independent variables  
- **Œµ**: Error term

  
---

### ‚öôÔ∏è Math Behind the Learning

### üìä **Cost Function in Regression**

In regression problems, the **cost function** (also known as **loss function**) measures the difference between the predicted values and the actual values. The goal is to minimize this cost function during model training to make the model's predictions as accurate as possible.

The most common cost functions used in regression are **Mean Squared Error (MSE)** and **Mean Absolute Error (MAE)**.

---

#### 1. **Mean Squared Error (MSE)**

MSE is the most commonly used cost function in regression. It measures the average of the squared differences between the predicted values and the actual values.

##### Formula:
$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

Where:
- y·µ¢ = Actual value
- yÃÇ·µ¢ = Predicted value
- n = Number of data points

---

#### 2. **Mean Absolute Error (MAE)**

MAE measures the average of the absolute differences between the predicted values and the actual values. It is less sensitive to outliers compared to MSE.

##### Formula:
$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

Where:
- y·µ¢ = Actual value
- yÃÇ·µ¢ = Predicted value
- n = Number of data points

---

### ‚ú® **Choosing the Right Cost Function**
- **MSE** is preferred when you want to heavily penalize large errors, as it squares the errors.
- **MAE** is preferred when you want to treat all errors equally and are more robust to outliers.


  
# Derivatives and Chain Rule in Machine Learning

In machine learning, the objective is to minimize a **cost function** (or loss function), which measures how far the model's predictions are from the actual values. To achieve this, we compute the **derivative** of the cost function with respect to each model parameter (such as weights and biases). These derivatives help determine how to update the parameters to reduce prediction error.

This concept is especially important in optimization algorithms like **gradient descent**, where the model parameters are adjusted iteratively using the gradient (i.e., the derivative) of the cost function.

---

## üîπ Example: Derivatives in Linear Regression

Consider a simple linear regression model:
yÃÇ = w‚ÇÅx + w‚ÇÄ


Where:
- `yÃÇ` is the predicted value,
- `x` is the input feature,
- `w‚ÇÅ` is the weight (slope),
- `w‚ÇÄ` is the bias (intercept).

The **Mean Squared Error (MSE)** is commonly used as the cost function:

MSE = (1/n) * Œ£(y·µ¢ - yÃÇ·µ¢)¬≤

Where:
- `y·µ¢` is the actual value,
- `yÃÇ·µ¢` is the predicted value.

To minimize the MSE, we compute the partial derivatives with respect to each parameter. For instance, the derivative of MSE with respect to `w‚ÇÅ` is:

‚àÇMSE/‚àÇw‚ÇÅ = -(2/n) * Œ£ x·µ¢(y·µ¢ - yÃÇ·µ¢)
---

This **gradient** tells us the direction and magnitude to update `w‚ÇÅ` in order to reduce error, which is repeatedly applied in gradient descent.

---

## üîπ Chain Rule in Machine Learning

The **chain rule** in calculus is used to compute the derivative of **composite functions**, which is common in machine learning when multiple functions are nested (e.g., in neural networks).

In linear regression, the prediction `yÃÇ·µ¢` is a function of the weights, and the cost function is a function of these predictions. To compute the derivative of the cost function with respect to weights, we use the chain rule.

### Example using Chain Rule:

Given:

yÃÇ·µ¢ = w‚ÇÅx·µ¢ + w‚ÇÄ
MSE = (1/n) * Œ£(y·µ¢ - yÃÇ·µ¢)¬≤


To compute the gradient with respect to `w‚ÇÅ`, apply the chain rule:

‚àÇMSE/‚àÇw‚ÇÅ = ‚àÇMSE/‚àÇyÃÇ·µ¢ * ‚àÇyÃÇ·µ¢/‚àÇw‚ÇÅ


Where:
- `‚àÇMSE/‚àÇyÃÇ·µ¢` is the derivative of the cost with respect to the predicted value,
- `‚àÇyÃÇ·µ¢/‚àÇw‚ÇÅ` is how the prediction changes with the weight `w‚ÇÅ`.

By breaking the derivative into smaller parts using the chain rule, we simplify the computation and make it easier to apply gradient descent.

---

# üìâ Gradient Descent in Machine Learning

**Gradient Descent** is an optimization algorithm used to minimize a **cost function** by iteratively updating model parameters (like weights and biases) in the direction of the steepest descent ‚Äî i.e., the **negative gradient** of the cost function.

It is the backbone of optimization in machine learning and deep learning models.

---

## üßÆ Mathematical Intuition

In a supervised learning setup like **linear regression**, we try to fit a line:

≈∑ = w‚ÇÅx + w‚ÇÄ

Where:
- `≈∑` is the predicted output,
- `w‚ÇÅ` is the weight (slope),
- `w‚ÇÄ` is the bias (intercept),
- `x` is the input feature.

We define the **cost function** to measure the difference between predicted values and actual values. A common choice is **Mean Squared Error (MSE)**:

J(w‚ÇÅ, w‚ÇÄ) = (1/n) * Œ£ (y·µ¢ - ≈∑·µ¢)¬≤
= (1/n) * Œ£ (y·µ¢ - (w‚ÇÅx·µ¢ + w‚ÇÄ))¬≤


---

## üîÅ Gradient Descent Algorithm

To minimize `J(w‚ÇÅ, w‚ÇÄ)`, we compute the **partial derivatives**:


‚àÇJ/‚àÇw‚ÇÅ = -(2/n) * Œ£ x·µ¢ (y·µ¢ - ≈∑·µ¢)
‚àÇJ/‚àÇw‚ÇÄ = -(2/n) * Œ£ (y·µ¢ - ≈∑·µ¢)


We then update the weights using a small constant called the **learning rate** `Œ±`:

w‚ÇÅ := w‚ÇÅ - Œ± * ‚àÇJ/‚àÇw‚ÇÅ
w‚ÇÄ := w‚ÇÄ - Œ± * ‚àÇJ/‚àÇw‚ÇÄ


Repeat the updates until the model converges ‚Äî i.e., the cost stops decreasing significantly.

---

## üß† Intuition

- If the gradient is **positive** ‚Üí decrease the weight to reduce error.
- If the gradient is **negative** ‚Üí increase the weight to reduce error.
- The learning rate `Œ±` controls how large a step we take during updates.

---

## ‚úÖ Applications

Gradient Descent allows machine learning models to **learn from data** by minimizing the cost function. It‚Äôs a foundational technique used in:

- üîπ Linear Regression  
- üîπ Logistic Regression  
- üîπ Neural Networks  
- üîπ Deep Learning  

---

> üîÅ Gradient Descent is like rolling a ball downhill ‚Äî you take steps proportional to the slope of the hill (gradient) until you reach the bottom (minimum error).


---

## üß™ Model Evaluation

Evaluating your machine learning model's performance is critical to ensure it's learning correctly and generalizing well to unseen data. This section provides scripts and explanations for splitting data and evaluating regression models using standard metrics.

---

### `train-test-split`

**Purpose**:  
Split your dataset into **training** and **testing** sets using `train_test_split` from `sklearn.model_selection`.

**Why?**  
- Prevents **overfitting** by evaluating on data the model hasn't seen during training.
- Helps assess how well the model will perform in real-world scenarios.

**Example Code**:
```python
from sklearn.model_selection import train_test_split

# X = features, y = target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 80% for training, 20% for testing

## üìä Regression Evaluation Metrics

Evaluate the performance of your regression model using the following metrics:

---

### 1Ô∏è‚É£ Mean Absolute Error (MAE)

- **Formula**: MAE = (1/n) √ó Œ£ | y·µ¢ - ≈∑·µ¢ |
- **Intuition**: Measures the average magnitude of errors in a set of predictions, without considering their direction.
- **Interpretation**: Lower MAE indicates better model performance.
- **Unit**: Same as the target variable.

---

### 2Ô∏è‚É£ Mean Squared Error (MSE)

- **Formula**: MSE = (1/n) √ó Œ£ ( y·µ¢ - ≈∑·µ¢ )¬≤
- **Intuition**: Like MAE, but squares the errors to penalize larger mistakes.
- **Interpretation**: Lower MSE indicates better accuracy.
- **Unit**: Squared unit of the target variable.

---

### 3Ô∏è‚É£ Root Mean Squared Error (RMSE)

- **Formula**: RMSE = ‚àöMSE
- **Intuition**: Square root of MSE; easier to interpret since it's in the same units as the target variable.
- **Interpretation**: Lower RMSE indicates better performance.
- **Unit**: Same as the target variable.

---

### 4Ô∏è‚É£ R¬≤ Score (Coefficient of Determination)

- **Formula**: R¬≤ = 1 - (SS_res / SS_tot)
- **Intuition**: Measures how well the regression predictions approximate the actual data.
- **Range**:  
  - `1.0`: Perfect predictions  
  - `0.0`: No better than predicting the mean  
  - `< 0`: Worse than predicting the mean  
- **Interpretation**: Higher R¬≤ indicates better model fit.
- **Unit**: Unitless

---

‚úÖ These metrics give you insight into how accurate and reliable your regression model is.

---

## üß™ Example Code

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Example data
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]

# Evaluation Metrics
mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_true, y_pred)

# Results
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R¬≤ Score: {r2:.2f}")


---

### üîß Data Preprocessing

- **`data-preprocessing.py`**  
  - **One-Hot Encoding** for categorical features  
  - **StandardScaler** for normalization  
  These preprocessing steps ensure your model learns effectively and fairly.

---

### üßÆ Advanced Regression

- **`polynomial-regression.ipynb`**  
  Learn how to fit **non-linear data** using polynomial features. Understand when and how to use this approach.

---

### ‚ö†Ô∏è Overfitting vs Underfitting

- **`overfitting-underfitting.md`**  
  Know the signs of underfitting (too simple) and overfitting (too complex). Learn to diagnose your model‚Äôs behavior.

- **`overfitting-remedies.py`**  
  Practical techniques to solve both problems ‚Äî from cross-validation to regularization and adding more data.

---

### üìè Regularization Techniques

- **`l1-l2-regularization.py`**  
  Apply **L1 (Lasso)** and **L2 (Ridge)** regularization in your regression models. Learn how these help prevent overfitting.

---

### ‚öñÔ∏è Bias-Variance Tradeoff

- **`bias-variance-tradeoff.md`**  
  Explore one of the most important concepts in machine learning:  
  **Bias (error due to underfitting) vs. Variance (error due to overfitting)** ‚Äî and how to balance them.

---

## üìÇ Folder Structure

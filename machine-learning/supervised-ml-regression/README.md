# ğŸ“Š Supervised Machine Learning â€” Regression

Welcome to the **Supervised Regression** section of my AI learning journey!  
This folder covers the complete theory and hands-on implementation of regression models using Python, from the very basics to regularization and model evaluation. ğŸš€

---

## ğŸ“š Topics Covered

Each topic below is backed with Python scripts, Jupyter notebooks, or Markdown notes.  
You can explore, run, or modify the code to understand how supervised regression truly works.

---

### âœ… Basics of Regression

- **`simple-linear-regression.py`**  
  ### **Linear Regression**

- Linear Regression is a supervised machine learning algorithm used to model the relationship between a **dependent variable** (target) and an **independent variable** (feature).
- In **single variable linear regression**, there is only **one input feature** and **one output variable**.
- The model aims to fit a **line of best fit** using the equation:Y=mX+c

   ![Regression Graph](https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg)

    
    Y = m X + c
    
    Where:
    
    - Y: Predicted output (target)
    - X: Input feature
    - m: Slope of the line (coefficient)
    - c: Y-intercept (constant)
 
  

## ğŸ“˜ Multiple Linear Regression

**Multiple Linear Regression** is a statistical technique that models the relationship between one **dependent variable** and two or more **independent variables**.

### ğŸ§® Equation:
y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™ + Îµ

![Image Description](https://miro.medium.com/v2/resize:fit:560/1*HLN6FxlXrzDtYN0KAlom4A.png)

### ğŸ“Œ Where:
- **y**: Dependent variable  
- **xâ‚, xâ‚‚, ..., xâ‚™**: Independent variables  
- **Î²â‚€**: Intercept  
- **Î²â‚, Î²â‚‚, ..., Î²â‚™**: Coefficients of the independent variables  
- **Îµ**: Error term

  
---

### âš™ï¸ Math Behind the Learning

### ğŸ“Š **Cost Function in Regression**

In regression problems, the **cost function** (also known as **loss function**) measures the difference between the predicted values and the actual values. The goal is to minimize this cost function during model training to make the model's predictions as accurate as possible.

The most common cost functions used in regression are **Mean Squared Error (MSE)** and **Mean Absolute Error (MAE)**.

---

#### 1. **Mean Squared Error (MSE)**

MSE is the most commonly used cost function in regression. It measures the average of the squared differences between the predicted values and the actual values.

##### Formula:
$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

Where:
- yáµ¢ = Actual value
- yÌ‚áµ¢ = Predicted value
- n = Number of data points

---

#### 2. **Mean Absolute Error (MAE)**

MAE measures the average of the absolute differences between the predicted values and the actual values. It is less sensitive to outliers compared to MSE.

##### Formula:
$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

Where:
- yáµ¢ = Actual value
- yÌ‚áµ¢ = Predicted value
- n = Number of data points

---

### âœ¨ **Choosing the Right Cost Function**
- **MSE** is preferred when you want to heavily penalize large errors, as it squares the errors.
- **MAE** is preferred when you want to treat all errors equally and are more robust to outliers.


  
# Derivatives and Chain Rule in Machine Learning

In machine learning, the objective is to minimize a **cost function** (or loss function), which measures how far the model's predictions are from the actual values. To achieve this, we compute the **derivative** of the cost function with respect to each model parameter (such as weights and biases). These derivatives help determine how to update the parameters to reduce prediction error.

This concept is especially important in optimization algorithms like **gradient descent**, where the model parameters are adjusted iteratively using the gradient (i.e., the derivative) of the cost function.

---

## ğŸ”¹ Example: Derivatives in Linear Regression

Consider a simple linear regression model:
yÌ‚ = wâ‚x + wâ‚€


Where:
- `yÌ‚` is the predicted value,
- `x` is the input feature,
- `wâ‚` is the weight (slope),
- `wâ‚€` is the bias (intercept).

The **Mean Squared Error (MSE)** is commonly used as the cost function:

MSE = (1/n) * Î£(yáµ¢ - yÌ‚áµ¢)Â²

Where:
- `yáµ¢` is the actual value,
- `yÌ‚áµ¢` is the predicted value.

To minimize the MSE, we compute the partial derivatives with respect to each parameter. For instance, the derivative of MSE with respect to `wâ‚` is:

âˆ‚MSE/âˆ‚wâ‚ = -(2/n) * Î£ xáµ¢(yáµ¢ - yÌ‚áµ¢)
---

This **gradient** tells us the direction and magnitude to update `wâ‚` in order to reduce error, which is repeatedly applied in gradient descent.

---

## ğŸ”¹ Chain Rule in Machine Learning

The **chain rule** in calculus is used to compute the derivative of **composite functions**, which is common in machine learning when multiple functions are nested (e.g., in neural networks).

In linear regression, the prediction `yÌ‚áµ¢` is a function of the weights, and the cost function is a function of these predictions. To compute the derivative of the cost function with respect to weights, we use the chain rule.

### Example using Chain Rule:

Given:

yÌ‚áµ¢ = wâ‚xáµ¢ + wâ‚€
MSE = (1/n) * Î£(yáµ¢ - yÌ‚áµ¢)Â²


To compute the gradient with respect to `wâ‚`, apply the chain rule:

âˆ‚MSE/âˆ‚wâ‚ = âˆ‚MSE/âˆ‚yÌ‚áµ¢ * âˆ‚yÌ‚áµ¢/âˆ‚wâ‚


Where:
- `âˆ‚MSE/âˆ‚yÌ‚áµ¢` is the derivative of the cost with respect to the predicted value,
- `âˆ‚yÌ‚áµ¢/âˆ‚wâ‚` is how the prediction changes with the weight `wâ‚`.

By breaking the derivative into smaller parts using the chain rule, we simplify the computation and make it easier to apply gradient descent.

---

# ğŸ“‰ Gradient Descent in Machine Learning

**Gradient Descent** is an optimization algorithm used to minimize a **cost function** by iteratively updating model parameters (like weights and biases) in the direction of the steepest descent â€” i.e., the **negative gradient** of the cost function.

It is the backbone of optimization in machine learning and deep learning models.

---

## ğŸ§® Mathematical Intuition

In a supervised learning setup like **linear regression**, we try to fit a line:

Å· = wâ‚x + wâ‚€

Where:
- `Å·` is the predicted output,
- `wâ‚` is the weight (slope),
- `wâ‚€` is the bias (intercept),
- `x` is the input feature.

We define the **cost function** to measure the difference between predicted values and actual values. A common choice is **Mean Squared Error (MSE)**:

J(wâ‚, wâ‚€) = (1/n) * Î£ (yáµ¢ - Å·áµ¢)Â²
= (1/n) * Î£ (yáµ¢ - (wâ‚xáµ¢ + wâ‚€))Â²


---

## ğŸ” Gradient Descent Algorithm

To minimize `J(wâ‚, wâ‚€)`, we compute the **partial derivatives**:


âˆ‚J/âˆ‚wâ‚ = -(2/n) * Î£ xáµ¢ (yáµ¢ - Å·áµ¢)
âˆ‚J/âˆ‚wâ‚€ = -(2/n) * Î£ (yáµ¢ - Å·áµ¢)


We then update the weights using a small constant called the **learning rate** `Î±`:

wâ‚ := wâ‚ - Î± * âˆ‚J/âˆ‚wâ‚
wâ‚€ := wâ‚€ - Î± * âˆ‚J/âˆ‚wâ‚€


Repeat the updates until the model converges â€” i.e., the cost stops decreasing significantly.

---

## ğŸ§  Intuition

- If the gradient is **positive** â†’ decrease the weight to reduce error.
- If the gradient is **negative** â†’ increase the weight to reduce error.
- The learning rate `Î±` controls how large a step we take during updates.

---

## âœ… Applications

Gradient Descent allows machine learning models to **learn from data** by minimizing the cost function. Itâ€™s a foundational technique used in:

- ğŸ”¹ Linear Regression  
- ğŸ”¹ Logistic Regression  
- ğŸ”¹ Neural Networks  
- ğŸ”¹ Deep Learning  

---

> ğŸ” Gradient Descent is like rolling a ball downhill â€” you take steps proportional to the slope of the hill (gradient) until you reach the bottom (minimum error).


---

### ğŸ¤” Key Insights

- **`why-mse-not-mae.md`**  
  Explore the **mathematical reasoning** behind using MSE over MAE during model training and optimization.

---

### ğŸ§ª Model Evaluation

- **`train-test-split.py`**  
  Learn how to properly split your dataset for training and testing. Prevents overfitting and ensures fair evaluation.

- **`model-evaluation-metrics.py`**  
  Understand and calculate key metrics:  
  - Mean Absolute Error (MAE)  
  - Mean Squared Error (MSE)  
  - Root Mean Squared Error (RMSE)  
  - RÂ² Score

---

### ğŸ”§ Data Preprocessing

- **`data-preprocessing.py`**  
  - **One-Hot Encoding** for categorical features  
  - **StandardScaler** for normalization  
  These preprocessing steps ensure your model learns effectively and fairly.

---

### ğŸ§® Advanced Regression

- **`polynomial-regression.ipynb`**  
  Learn how to fit **non-linear data** using polynomial features. Understand when and how to use this approach.

---

### âš ï¸ Overfitting vs Underfitting

- **`overfitting-underfitting.md`**  
  Know the signs of underfitting (too simple) and overfitting (too complex). Learn to diagnose your modelâ€™s behavior.

- **`overfitting-remedies.py`**  
  Practical techniques to solve both problems â€” from cross-validation to regularization and adding more data.

---

### ğŸ“ Regularization Techniques

- **`l1-l2-regularization.py`**  
  Apply **L1 (Lasso)** and **L2 (Ridge)** regularization in your regression models. Learn how these help prevent overfitting.

---

### âš–ï¸ Bias-Variance Tradeoff

- **`bias-variance-tradeoff.md`**  
  Explore one of the most important concepts in machine learning:  
  **Bias (error due to underfitting) vs. Variance (error due to overfitting)** â€” and how to balance them.

---

## ğŸ“‚ Folder Structure
